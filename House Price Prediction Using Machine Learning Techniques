{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sevdenurgenc/house-price-eda-and-prediction-with-ml?scriptVersionId=171187331\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# House Price Prediction Using Machine Learning Techniques\n\n## Summary \n\nThis project aims to develop a prediction model for house prices using machine learning techniques. It is aimed to accurately estimate the sales price of houses by taking advantage of various features of the house data, such as overall quality, square footage, and neighborhood, the goal is to accurately. Through data pre-processing, feature engineering, model selection and evaluation, the project aims to deliver valuable insights and predictions for stakeholders in the real estate industry.\n\n\n## Business Objective\nThe main goal of this project is to help everyone in real estate, especially buyers, sellers, and investors, make better decisions when buying or selling properties. We aim to develop a model that can accurately predict house prices by looking at key features like location, size, and quality. This will support them in setting prices, making investments, and assessing risks. Ultimately, our project seeks to make the real estate market more open and effective.\n\n## Description of Sections\n1-Setup and Library Installation: Installing essential libraries like numpy, pandas, matplotlib, seaborn, and sklearn for data analysis and modeling.\n2-Data Preprocessing and Exploration: Loading the dataset into a pandas DataFrame, exploring its structure and features, and dealing with missing values and outliers.\n3-Data Cleaning: Preparing the dataset for analysis by converting data types, filling in missing values, and removing unnecessary columns.\n4-Feature Engineering: Creating new features from existing ones to improve the model's predictive capability.\n5-Data Transformation: Transforming data by correcting skewness and encoding categorical variables for modeling.\n6-Model Selection and Training: Comparing various machine learning models' performance and selecting the best one for training.\n7-Model Evaluation: Assessing the chosen model's performance using cross-validation and calculating metrics like Mean Absolute Error (MAE) and R-squared (R²).\n8-Training Top Models: Training the best-performing models and evaluating their performance on both the base and final datasets.\n\n## Results\nThe project successfully developed a predictive model for house prices, achieving good performance metrics such as low MAE and high R² values. The selected model provides valuable information for the real estate industry by demonstrating robustness and accuracy in predicting house prices.\n","metadata":{}},{"cell_type":"markdown","source":"# Setup Environment","metadata":{}},{"cell_type":"code","source":"pip install pycaret","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-09T15:13:28.463308Z","iopub.execute_input":"2024-04-09T15:13:28.463967Z","iopub.status.idle":"2024-04-09T15:14:12.38706Z","shell.execute_reply.started":"2024-04-09T15:13:28.463929Z","shell.execute_reply":"2024-04-09T15:14:12.385787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade scikit-learn","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-09T15:14:12.389556Z","iopub.execute_input":"2024-04-09T15:14:12.390395Z","iopub.status.idle":"2024-04-09T15:14:24.97557Z","shell.execute_reply.started":"2024-04-09T15:14:12.390348Z","shell.execute_reply":"2024-04-09T15:14:24.974445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Models\nfrom sklearn.linear_model import BayesianRidge, OrthogonalMatchingPursuit\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport xgboost as xgb\n\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\n\n\n# Model Selection\nfrom pycaret.regression import setup, compare_models\n\n# pd.set_option('display.max_rows', None) # Disable truncation of isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:24.976959Z","iopub.execute_input":"2024-04-09T15:14:24.977258Z","iopub.status.idle":"2024-04-09T15:14:30.666283Z","shell.execute_reply.started":"2024-04-09T15:14:24.977228Z","shell.execute_reply":"2024-04-09T15:14:30.665453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Assign dataset to df\n\ndf = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:30.66807Z","iopub.execute_input":"2024-04-09T15:14:30.668903Z","iopub.status.idle":"2024-04-09T15:14:30.738891Z","shell.execute_reply.started":"2024-04-09T15:14:30.668873Z","shell.execute_reply":"2024-04-09T15:14:30.738117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore Data","metadata":{}},{"cell_type":"code","source":"# Check data shape\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:30.740673Z","iopub.execute_input":"2024-04-09T15:14:30.74172Z","iopub.status.idle":"2024-04-09T15:14:30.749575Z","shell.execute_reply.started":"2024-04-09T15:14:30.741662Z","shell.execute_reply":"2024-04-09T15:14:30.748528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:30.75133Z","iopub.execute_input":"2024-04-09T15:14:30.75172Z","iopub.status.idle":"2024-04-09T15:14:30.77947Z","shell.execute_reply.started":"2024-04-09T15:14:30.751666Z","shell.execute_reply":"2024-04-09T15:14:30.778765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-09T15:14:30.780408Z","iopub.execute_input":"2024-04-09T15:14:30.780861Z","iopub.status.idle":"2024-04-09T15:14:30.78711Z","shell.execute_reply.started":"2024-04-09T15:14:30.780834Z","shell.execute_reply":"2024-04-09T15:14:30.786247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-09T15:14:30.788163Z","iopub.execute_input":"2024-04-09T15:14:30.788489Z","iopub.status.idle":"2024-04-09T15:14:30.810708Z","shell.execute_reply.started":"2024-04-09T15:14:30.788465Z","shell.execute_reply":"2024-04-09T15:14:30.809865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data has 43 objects, 35 integers and 3 float data.","metadata":{}},{"cell_type":"code","source":"# Display the sum of null values for each column\ndf.isnull().sum()\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-09T15:14:30.811924Z","iopub.execute_input":"2024-04-09T15:14:30.812277Z","iopub.status.idle":"2024-04-09T15:14:30.822699Z","shell.execute_reply.started":"2024-04-09T15:14:30.812252Z","shell.execute_reply":"2024-04-09T15:14:30.821613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:30.825888Z","iopub.execute_input":"2024-04-09T15:14:30.826168Z","iopub.status.idle":"2024-04-09T15:14:30.839619Z","shell.execute_reply.started":"2024-04-09T15:14:30.826143Z","shell.execute_reply":"2024-04-09T15:14:30.838152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize NA values\nmissing = df.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nplt.figure(figsize=(15,8))\nmissing.plot.bar()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:30.842759Z","iopub.execute_input":"2024-04-09T15:14:30.843039Z","iopub.status.idle":"2024-04-09T15:14:31.250987Z","shell.execute_reply.started":"2024-04-09T15:14:30.843015Z","shell.execute_reply":"2024-04-09T15:14:31.249994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize NA values in test data frame\nmissing = df_test.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nplt.figure(figsize=(15,8))\nmissing.plot.bar()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.252286Z","iopub.execute_input":"2024-04-09T15:14:31.25284Z","iopub.status.idle":"2024-04-09T15:14:31.770097Z","shell.execute_reply.started":"2024-04-09T15:14:31.252809Z","shell.execute_reply":"2024-04-09T15:14:31.769348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Important point :Issues Found \n\n- Some NA values in the dataset appear as missing data but actually represent assigned categories. To solve this problem, we will convert non-numeric NAs to categorical 'None' values, while numeric NAs will be assigned using the KNN Regressor.\n\n- The 'MSSubClass' property is currently kept as an integer but needs to be treated as categorical. Accordingly, we will convert the data type to object.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"## Convert MSSubClass to str data type","metadata":{}},{"cell_type":"code","source":"df['MSSubClass'] = df['MSSubClass'].astype('str') # Convert MSSubClass to str object\ndf_test['MSSubClass'] = df['MSSubClass'].astype('str') # Convert MSSubClass to str object\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.771069Z","iopub.execute_input":"2024-04-09T15:14:31.771757Z","iopub.status.idle":"2024-04-09T15:14:31.778647Z","shell.execute_reply.started":"2024-04-09T15:14:31.771714Z","shell.execute_reply":"2024-04-09T15:14:31.777744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = df.copy()\ndf_test2 = df_test.copy()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.779813Z","iopub.execute_input":"2024-04-09T15:14:31.780311Z","iopub.status.idle":"2024-04-09T15:14:31.792404Z","shell.execute_reply.started":"2024-04-09T15:14:31.780245Z","shell.execute_reply":"2024-04-09T15:14:31.791123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How to fill Categorical Values:\n\nReplace categorical values with assigned descriptions with 'None'. For other categorical values, use the mode of the respective column for filling missing values.\n","metadata":{}},{"cell_type":"code","source":"# Impute using a constant value - If NA value is included in description as a category, fill it with 'None'\nfor column in ['Alley',\n               'BsmtQual',\n               'BsmtCond',\n               'BsmtExposure',\n               'BsmtFinType1',\n               'BsmtFinType2',\n               'FireplaceQu',\n               'GarageType', \n               'GarageFinish',\n               'GarageQual',\n               'GarageCond',\n               'PoolQC',\n               'Fence',\n               'MiscFeature',\n]:\n    df2[column] = df2[column].fillna('None')\n\n# Impute using the column mode - If NA value is not included in description, fill with mode\nfor column in ['MSZoning',\n               'Utilities',\n               'Exterior1st',\n               'Exterior2nd',\n               'MasVnrType',\n               'Electrical',\n               'KitchenQual',\n               'Functional',\n               'SaleType'    \n]:\n    df2[column] = df2[column].fillna(df2[column].mode()[0]) ## We use [0] because mode can return multiple values","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.793871Z","iopub.execute_input":"2024-04-09T15:14:31.794805Z","iopub.status.idle":"2024-04-09T15:14:31.816817Z","shell.execute_reply.started":"2024-04-09T15:14:31.794767Z","shell.execute_reply":"2024-04-09T15:14:31.81575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute using a constant value - If NA value is included in description as a category, fill it with 'None'\nfor column in ['Alley',\n               'BsmtQual',\n               'BsmtCond',\n               'BsmtExposure',\n               'BsmtFinType1',\n               'BsmtFinType2',\n               'FireplaceQu',\n               'GarageType', \n               'GarageFinish',\n               'GarageQual',\n               'GarageCond',\n               'PoolQC',\n               'Fence',\n               'MiscFeature',\n]:\n    df_test2[column] = df_test2[column].fillna('None')\n\n# Impute using the column mode - If NA value is not included in description, fill with mode\nfor column in ['MSZoning',\n               'Utilities',\n               'Exterior1st',\n               'Exterior2nd',\n               'MasVnrType',\n               'Electrical',\n               'KitchenQual',\n               'Functional',\n               'SaleType'    \n]:\n    df_test2[column] = df_test2[column].fillna(df_test2[column].mode()[0]) ## We use [0] because mode can return multiple values","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.818035Z","iopub.execute_input":"2024-04-09T15:14:31.818339Z","iopub.status.idle":"2024-04-09T15:14:31.839417Z","shell.execute_reply.started":"2024-04-09T15:14:31.818314Z","shell.execute_reply":"2024-04-09T15:14:31.838377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Return all 'object' columns with at least 1 missing value\n\ndf2.select_dtypes('object').loc[:, df2.isna().sum() > 0].columns ","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.840733Z","iopub.execute_input":"2024-04-09T15:14:31.841618Z","iopub.status.idle":"2024-04-09T15:14:31.857887Z","shell.execute_reply.started":"2024-04-09T15:14:31.841581Z","shell.execute_reply":"2024-04-09T15:14:31.856931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test2.select_dtypes('object').loc[:, df_test2.isna().sum() > 0].columns ","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.858928Z","iopub.execute_input":"2024-04-09T15:14:31.859208Z","iopub.status.idle":"2024-04-09T15:14:31.874641Z","shell.execute_reply.started":"2024-04-09T15:14:31.859184Z","shell.execute_reply":"2024-04-09T15:14:31.873744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Check NAs of Object data types after imputation\n\ndf2.select_dtypes('object').isna().sum()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-09T15:14:31.875864Z","iopub.execute_input":"2024-04-09T15:14:31.876613Z","iopub.status.idle":"2024-04-09T15:14:31.890304Z","shell.execute_reply.started":"2024-04-09T15:14:31.876584Z","shell.execute_reply":"2024-04-09T15:14:31.889363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test2.select_dtypes('object').isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.891426Z","iopub.execute_input":"2024-04-09T15:14:31.89174Z","iopub.status.idle":"2024-04-09T15:14:31.904724Z","shell.execute_reply.started":"2024-04-09T15:14:31.891706Z","shell.execute_reply":"2024-04-09T15:14:31.903918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Output Confirmation: Object NAs Filled\n\nThe output confirms that missing values in object-type columns have been successfully filled.\n","metadata":{}},{"cell_type":"markdown","source":"## Drop 'Id' Column","metadata":{}},{"cell_type":"code","source":"df2.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.905889Z","iopub.execute_input":"2024-04-09T15:14:31.906659Z","iopub.status.idle":"2024-04-09T15:14:31.913279Z","shell.execute_reply.started":"2024-04-09T15:14:31.906624Z","shell.execute_reply":"2024-04-09T15:14:31.912517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = df2.drop(['Id'], axis = 1)\ndf_test2 = df_test2.drop('Id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.91541Z","iopub.execute_input":"2024-04-09T15:14:31.915707Z","iopub.status.idle":"2024-04-09T15:14:31.9286Z","shell.execute_reply.started":"2024-04-09T15:14:31.915655Z","shell.execute_reply":"2024-04-09T15:14:31.927798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.929878Z","iopub.execute_input":"2024-04-09T15:14:31.93035Z","iopub.status.idle":"2024-04-09T15:14:31.960511Z","shell.execute_reply.started":"2024-04-09T15:14:31.930321Z","shell.execute_reply":"2024-04-09T15:14:31.959512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fill Missing Values for Numerical Features","metadata":{}},{"cell_type":"code","source":"df2.select_dtypes(np.number).isna().sum()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-09T15:14:31.961762Z","iopub.execute_input":"2024-04-09T15:14:31.962074Z","iopub.status.idle":"2024-04-09T15:14:31.970527Z","shell.execute_reply.started":"2024-04-09T15:14:31.962047Z","shell.execute_reply":"2024-04-09T15:14:31.969562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3 = df2.copy()\ndf_test3 = df_test2.copy()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.971785Z","iopub.execute_input":"2024-04-09T15:14:31.972056Z","iopub.status.idle":"2024-04-09T15:14:31.981812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neighborhood_counts = df3['Neighborhood'].value_counts()\nprint(neighborhood_counts)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.983072Z","iopub.execute_input":"2024-04-09T15:14:31.984036Z","iopub.status.idle":"2024-04-09T15:14:31.989613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Filling NAs with KNN Regressor\n\nIn this step, we utilize Scikit-Learn's KNN Regressor method to fill missing values (NAs) in numerical features. It's important to note that KNN regressor can only be applied to numerical values. Thus, we exclusively address numerical columns during this process.\n","metadata":{}},{"cell_type":"code","source":"def knn_impute(df, na_target):\n    df = df.copy()\n    \n    numeric_df = df.select_dtypes(np.number)\n    non_na_columns = numeric_df.loc[: ,numeric_df.isna().sum() == 0].columns ## Columns with no missing values\n    \n    y_train = numeric_df.loc[numeric_df[na_target].isna() == False, na_target] # All values of na_target that do not have missing values\n    X_train = numeric_df.loc[numeric_df[na_target].isna() == False, non_na_columns] # All values of non_na_columns that do not have missing values\n    X_test = numeric_df.loc[numeric_df[na_target].isna() == True, non_na_columns] #  Rest of the data that do not have missing values\n    \n    knn = KNeighborsRegressor() # Assign KneighborsRegressor() to knn\n    knn.fit(X_train, y_train) # Call regressor to fit missing values\n    \n    y_pred = knn.predict(X_test)\n    \n    df.loc[df[na_target].isna() == True, na_target] = y_pred # Fill missing values with y_train prediction based on KNeighbor Regression.\n    \n    # Basically, we filled all numerical missing values based on k-nearest neighbors of respective values and columns\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:31.990667Z","iopub.execute_input":"2024-04-09T15:14:31.991193Z","iopub.status.idle":"2024-04-09T15:14:32.001154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3.columns[df3.isna().sum() > 0]","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:32.008783Z","iopub.execute_input":"2024-04-09T15:14:32.009068Z","iopub.status.idle":"2024-04-09T15:14:32.020717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test3.columns[df_test3.isna().sum() > 0]","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:32.02209Z","iopub.execute_input":"2024-04-09T15:14:32.022459Z","iopub.status.idle":"2024-04-09T15:14:32.034851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The code provided below should be executed only once, as it fills all missing values in the dataset. Attempting to run it multiple times will result in errors if no missing values are found.","metadata":{}},{"cell_type":"code","source":"for column in [\n                'GarageYrBlt',\n                'LotFrontage',\n                'MasVnrArea'\n              ]:\n    df3 = knn_impute(df3, column)\n# Call KNeighborsRegressor() to fill NA values in numeric columns using k-nearest neighbors regression","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:32.035809Z","iopub.execute_input":"2024-04-09T15:14:32.036896Z","iopub.status.idle":"2024-04-09T15:14:32.21662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in ['LotFrontage',\n            'MasVnrArea',\n            'BsmtFinSF1',\n            'BsmtFinSF2',\n            'BsmtUnfSF',\n            'TotalBsmtSF',\n            'BsmtFullBath',\n            'BsmtHalfBath',\n            'GarageYrBlt',\n            'GarageCars',\n            'GarageArea'\n           ]:\n    df_test3 = knn_impute(df_test3, column)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:32.217777Z","iopub.execute_input":"2024-04-09T15:14:32.218077Z","iopub.status.idle":"2024-04-09T15:14:32.365082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3.isna().sum()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-09T15:14:32.366573Z","iopub.execute_input":"2024-04-09T15:14:32.367236Z","iopub.status.idle":"2024-04-09T15:14:32.379041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test3.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:32.380478Z","iopub.execute_input":"2024-04-09T15:14:32.381107Z","iopub.status.idle":"2024-04-09T15:14:32.395295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numerical Missing Values Filled Using KNN Regressor\n\nThe missing numerical values have been filled using the KNN Regressor method. The output above confirms that there are no more missing values remaining.\n","metadata":{}},{"cell_type":"markdown","source":"\n## Check Correlation","metadata":{}},{"cell_type":"code","source":"num_features = df3.select_dtypes(include=[np.number]) # Assign numeric columns\n\nnum_features.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:32.396654Z","iopub.execute_input":"2024-04-09T15:14:32.397283Z","iopub.status.idle":"2024-04-09T15:14:32.404187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features = df3.select_dtypes('object') # Assign categorical object columns\ncat_features.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:32.405519Z","iopub.execute_input":"2024-04-09T15:14:32.406133Z","iopub.status.idle":"2024-04-09T15:14:32.413797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlations List","metadata":{}},{"cell_type":"code","source":"correlation = num_features.corr() # Return correlation for numerical features\nprint(correlation['SalePrice'].sort_values(ascending = False)) # Print value correlation with SalePrice on a descending list","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-09T15:14:32.415035Z","iopub.execute_input":"2024-04-09T15:14:32.415505Z","iopub.status.idle":"2024-04-09T15:14:32.432234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlations Heatmap For All Numerical Columns","metadata":{}},{"cell_type":"code","source":"f , ax = plt.subplots(figsize = (14,21))\n\nplt.title('Correlation of Numeric Features with Sale Price', y=1, size=16 )\n\nsns.heatmap(correlation, square = True, vmax = 0.8)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:32.433656Z","iopub.execute_input":"2024-04-09T15:14:32.434273Z","iopub.status.idle":"2024-04-09T15:14:33.408549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lighter boxes have higher correlation, while darker boxes have lower correlation. Fully dark boxes indicate high negative correlation, which can also provide valuable insights.\n","metadata":{}},{"cell_type":"markdown","source":"### Correlation Heatmap For Highest Correlated Columns","metadata":{}},{"cell_type":"code","source":"k = 12\ncols = correlation.nlargest(k, 'SalePrice')['SalePrice'].index\nprint(cols)\ncm = np.corrcoef(df3[cols].values.T)\nf , ax = plt.subplots(figsize = (14, 12))\nsns.heatmap(cm, vmax=.8, linewidths=0.01, square=True, annot=True, cmap='viridis',\n           linecolor='white', xticklabels=cols.values, annot_kws={'size':12},yticklabels=cols.values)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:33.410341Z","iopub.execute_input":"2024-04-09T15:14:33.410743Z","iopub.status.idle":"2024-04-09T15:14:34.41048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We are using the top 12 columns since they have a correlation above 0.5 with SalePrice.\n\n- GarageCars and GarageArea show multicollinearity as they are correlated to each other with a rate of 0.88. We will remove GarageArea since it provides the same data as GarageCars, but GarageCars has a higher correlation with SalePrice. This way, we will increase model precision and reduce randomness.","metadata":{}},{"cell_type":"code","source":"df4 = df3.copy()\ndf4 = df4.drop('GarageArea', axis=1) # Drop GarageArea\ndf4.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:34.412098Z","iopub.execute_input":"2024-04-09T15:14:34.412495Z","iopub.status.idle":"2024-04-09T15:14:34.422459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test4 = df_test3.copy()\ndf_test4 = df_test4.drop('GarageArea', axis=1) # Drop GarageArea\ndf_test4.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:34.424233Z","iopub.execute_input":"2024-04-09T15:14:34.424627Z","iopub.status.idle":"2024-04-09T15:14:34.43385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scatter Plots","metadata":{}},{"cell_type":"code","source":"# Define colors for each plot\ncolors = ['blue', 'green', 'red', 'orange', 'purple', 'brown', 'pink', 'gray', 'crimson', 'brown']\n\n# Draw scatter plots of columns with highest correlation with target\nfig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(nrows=5, ncols=2, figsize=(14,15))\n\nsns.regplot(x='OverallQual', y='SalePrice', data=df4, scatter=True, fit_reg=True, ax=ax1, color=colors[0])\nsns.regplot(x='GrLivArea', y='SalePrice', data=df4, scatter=True, fit_reg=True, ax=ax2, color=colors[1])\nsns.regplot(x='GarageCars', y='SalePrice', data=df4, scatter=True, fit_reg=True, ax=ax3, color=colors[2])\nsns.regplot(x='TotalBsmtSF', y='SalePrice', data=df4, scatter=True, fit_reg=True, ax=ax4, color=colors[3])\nsns.regplot(x='1stFlrSF', y='SalePrice', data=df4, scatter=True, fit_reg=True, ax=ax5, color=colors[4])\nsns.regplot(x='FullBath', y='SalePrice', data=df4, scatter=True, fit_reg=True, ax=ax6, color=colors[5])\nsns.regplot(x='TotRmsAbvGrd', y='SalePrice', data=df4, scatter=True, fit_reg=True, ax=ax7, color=colors[6])\nsns.regplot(x='YearBuilt', y='SalePrice', data=df4, scatter=True, fit_reg=True, ax=ax8, color=colors[7])\nsns.regplot(x='GarageYrBlt', y='SalePrice', data=df4, scatter=True, fit_reg=True, ax=ax9, color=colors[8])\nsns.regplot(x='WoodDeckSF', y='SalePrice', data=df4, scatter=True, fit_reg=True, ax=ax10, color=colors[9])\n\nplt.tight_layout()  # Adjust subplot layout to prevent overlap\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:34.435306Z","iopub.execute_input":"2024-04-09T15:14:34.435784Z","iopub.status.idle":"2024-04-09T15:14:37.804986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysis Summary\n\nThe code generates scatter plots for the columns with the highest correlation with the target variable, 'SalePrice'. Each subplot represents a different feature plotted against the target variable.\n\nKey points from the scatter plots:\n\n1. **OverallQual vs. SalePrice**: There is a positive linear relationship between overall quality and sale price, indicating that higher quality properties tend to have higher sale prices.\n\n2. **GrLivArea vs. SalePrice**: There is a strong positive correlation between the above-ground living area and sale price, suggesting that larger living areas generally command higher prices.\n\n3. **GarageCars vs. SalePrice**: The number of cars that can fit into the garage shows a positive correlation with sale price, indicating that properties with larger garage capacities tend to have higher prices.\n\n4. **TotalBsmtSF vs. SalePrice**: Total basement square footage has a positive correlation with sale price, indicating that properties with larger basements tend to have higher prices.\n\n5. **1stFlrSF vs. SalePrice**: The square footage of the first floor shows a positive correlation with sale price, suggesting that larger first floors generally command higher prices.\n\n6. **FullBath vs. SalePrice**: The number of full bathrooms has a positive correlation with sale price, indicating that properties with more bathrooms tend to have higher prices.\n\n7. **TotRmsAbvGrd vs. SalePrice**: Total rooms above ground show a positive correlation with sale price, suggesting that properties with more rooms generally command higher prices.\n\n8. **YearBuilt vs. SalePrice**: There is a positive correlation between the year the property was built and sale price, indicating that newer properties tend to have higher prices.\n\n9. **GarageYrBlt vs. SalePrice**: The year the garage was built shows a positive correlation with sale price, suggesting that newer garages may contribute to higher property prices.\n\n10. **WoodDeckSF vs. SalePrice**: The square footage of wood deck area shows a positive correlation with sale price, indicating that properties with larger deck areas tend to have higher prices.\n\nOverall, these scatter plots provide insights into the relationship between various features and the target variable, helping to identify important predictors of sale price.\n","metadata":{}},{"cell_type":"markdown","source":"## Box Plots","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x=df4['SalePrice'], color = 'silver')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:37.806729Z","iopub.execute_input":"2024-04-09T15:14:37.807087Z","iopub.status.idle":"2024-04-09T15:14:37.9684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(16,10))\nfig = sns.boxplot(x='SaleType', y='SalePrice', data = df4)\nfig.axis(ymin=0, ymax=800000)\nxt = plt.xticks(rotation=0)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:37.969751Z","iopub.execute_input":"2024-04-09T15:14:37.970023Z","iopub.status.idle":"2024-04-09T15:14:38.317653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(16,10))\nfig = sns.boxplot(x='OverallQual', y='SalePrice', data=df4)\nfig.axis(ymin=0, ymax=800000)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.31933Z","iopub.execute_input":"2024-04-09T15:14:38.319781Z","iopub.status.idle":"2024-04-09T15:14:38.690708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysis Summary of Box Plots\n\n### Box Plot for SalePrice Distribution\nThe first box plot shows the distribution of sale prices. The median sale price appears to be around 160,000 USD with a significant number of outliers indicating higher-priced properties. The interquartile range (IQR) suggests that the majority of properties fall within the 130,000 USD to 215,000 USD range.\n\n### Box Plot for SaleType vs. SalePrice\nThis box plot compares the sale prices across different sale types. Notable observations include:\n- Properties sold through New Homes (New) tend to have higher median sale prices compared to other sale types.\n- Sale prices for Court Officer Deed/Estate (CWD) and Condominiums (Con) show considerable variability, with a wider range of prices and numerous outliers.\n\n### Box Plot for OverallQual vs. SalePrice\nThis box plot examines how sale prices vary with overall quality ratings. Key insights include:\n- There is a clear positive correlation between overall quality and sale price, with higher quality properties commanding higher prices.\n- Properties with an overall quality rating of 10 have the highest median sale price, indicating that exceptional quality leads to higher property values.\n- As overall quality increases, the number of outliers also increases, suggesting that higher-quality properties may exhibit greater variability in sale prices.\n\nOverall, these box plots provide valuable insights into the distribution of sale prices and how they vary across different sale types and property quality ratings.\n","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering\n\n### 1. Neighborhood Score\nWe will create a new feature called \"Neighborhood Score\" based on the average sale price of properties within each neighborhood. This score will provide insight into the desirability and perceived value of properties in different neighborhoods.\n\n### 2. House Age\nThe \"House Age\" feature will represent the age of each property by subtracting the year it was built from the current year. This will provide information on the relative age of the property, which can be a significant factor in determining its value.\n\n### 3. SqFtPerRoom\nThe \"SqFtPerRoom\" feature will calculate the average square footage per room in each property. This metric can indicate the spaciousness and layout efficiency of a home, which can influence its perceived value.\n\n### 4. Total Home Quality\nWe will aggregate the quality ratings of various features (such as OverallQual, ExterQual, KitchenQual, etc.) to create a composite \"Total Home Quality\" score. This score will provide a comprehensive assessment of the overall quality of the property.\n\n### 5. Total Bathrooms\nThe \"Total Bathrooms\" feature will sum the number of full bathrooms and half bathrooms in each property. This metric will capture the overall bathroom count, which is an essential factor for many homebuyers.\n\n### 6. High Quality SqFt\nThe \"High Quality SqFt\" feature will represent the total square footage of high-quality living spaces, such as those with above-average quality ratings. This metric will highlight the premium living areas within each property, which can significantly impact its value.\n","metadata":{}},{"cell_type":"code","source":"df4['Neighborhoodscore'] = df4['Neighborhood'].map(df4['Neighborhood'].value_counts())\n\ndf4['HouseAge'] = df4['YrSold'] - df4['YearBuilt']\n\ndf4[\"SqFtPerRoom\"] = df4[\"GrLivArea\"] / (df4[\"TotRmsAbvGrd\"] + df4[\"FullBath\"] + df4[\"HalfBath\"] + df4[\"KitchenAbvGr\"])\n\ndf4['Total_Home_Quality'] = df4['OverallQual'] + df4['OverallCond']\n\ndf4['Total_Bathrooms'] = (df4['FullBath'] + (0.5 * df4['HalfBath']) + df4['BsmtFullBath'] + (0.5 * df4['BsmtHalfBath']))\n\ndf4[\"HighQualSF\"] = df4[\"1stFlrSF\"] + df4[\"2ndFlrSF\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.692381Z","iopub.execute_input":"2024-04-09T15:14:38.692797Z","iopub.status.idle":"2024-04-09T15:14:38.705329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test4['Neighborhoodscore'] = df_test4['Neighborhood'].map(df_test4['Neighborhood'].value_counts())\n                                                             \ndf_test4['HouseAge'] = df_test4['YrSold'] - df_test4['YearBuilt']\n\ndf_test4[\"SqFtPerRoom\"] = df_test4[\"GrLivArea\"] / (df_test4[\"TotRmsAbvGrd\"] + df_test4[\"FullBath\"] + df_test4[\"HalfBath\"] + df_test4[\"KitchenAbvGr\"])\n\ndf_test4['Total_Home_Quality'] = df_test4['OverallQual'] + df_test4['OverallCond']\n\ndf_test4['Total_Bathrooms'] = (df_test4['FullBath'] + (0.5 * df_test4['HalfBath']) + df_test4['BsmtFullBath'] + (0.5 * df_test4['BsmtHalfBath']))\n\ndf_test4[\"HighQualSF\"] = df_test4[\"1stFlrSF\"] + df_test4[\"2ndFlrSF\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.70639Z","iopub.execute_input":"2024-04-09T15:14:38.706658Z","iopub.status.idle":"2024-04-09T15:14:38.724315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Log Transform for Skewed Features\n\nTo address skewness in certain features, we will apply a log transformation. This transformation helps to normalize the distribution of skewed features, making them more suitable for linear models.\n\nWe will utilize three separate DataFrames for different stages of model development:\n\n1. `dfx`: This DataFrame includes transformations applied to the original dataset (`df`) and subsequent cleaning steps (`df4`, `df5`, `df6`, `df7`), and it will be used for the final model.\n\n2. `df5_b`: This DataFrame specifically focuses on creating a base model and incorporates transformations up to the `df5` stage.\n\n3. `df_testx`: This DataFrame comprises the test data and will be utilized for final model evaluation.\n\nBy segregating the data into these distinct DataFrames, we can effectively manage the feature engineering and modeling processes while ensuring consistency and reproducibility in our analysis.\n","metadata":{}},{"cell_type":"code","source":"# Assign target to SalePrice data\ntarget = df['SalePrice']","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.725258Z","iopub.execute_input":"2024-04-09T15:14:38.726063Z","iopub.status.idle":"2024-04-09T15:14:38.731578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop SalePrice from data frame\ndf4 = df4.drop(['SalePrice'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.732583Z","iopub.execute_input":"2024-04-09T15:14:38.732893Z","iopub.status.idle":"2024-04-09T15:14:38.742494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df5 = df4.copy()\ndf_test5 = df_test4.copy()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.743653Z","iopub.execute_input":"2024-04-09T15:14:38.744027Z","iopub.status.idle":"2024-04-09T15:14:38.754079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df5_b = df4.copy()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.755219Z","iopub.execute_input":"2024-04-09T15:14:38.755577Z","iopub.status.idle":"2024-04-09T15:14:38.762541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check Skeweness","metadata":{}},{"cell_type":"code","source":"skew_df = pd.DataFrame(df5.select_dtypes(np.number).columns, columns=['Feature'])\nskew_df['Skew'] = skew_df['Feature'].apply(lambda feature: scipy.stats.skew(df5[feature]))\nskew_df['Absolute Skew'] = skew_df['Skew'].apply(abs)\nskew_df['Skewed'] = skew_df['Absolute Skew'].apply(lambda x: True if x >= 0.5 else False)\nskew_df","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.763764Z","iopub.execute_input":"2024-04-09T15:14:38.764121Z","iopub.status.idle":"2024-04-09T15:14:38.813032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reduce skeweness using log1 scale\nfor col in skew_df.query('Skewed == True')['Feature'].values:\n    df5[col] = np.log1p(df5[col]) # We use log1p since data has 0 values and log will not work on this case","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.814312Z","iopub.execute_input":"2024-04-09T15:14:38.814716Z","iopub.status.idle":"2024-04-09T15:14:38.832916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skew_df = pd.DataFrame(df5.select_dtypes(np.number).columns, columns=['Feature'])\nskew_df['Skew'] = skew_df['Feature'].apply(lambda feature: scipy.stats.skew(df5[feature]))\nskew_df['Absolute Skew'] = skew_df['Skew'].apply(abs)\nskew_df['Skewed'] = skew_df['Absolute Skew'].apply(lambda x: True if x >= 0.5 else False)\nskew_df","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.834865Z","iopub.execute_input":"2024-04-09T15:14:38.835264Z","iopub.status.idle":"2024-04-09T15:14:38.883766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding Categorical Variables\n\nIn this section, we address the handling of categorical variables within the dataset. Categorical variables, which represent qualitative data and often take on non-numeric values, require special treatment for effective integration into machine learning models.\n\nOur approach involves encoding these categorical variables into numerical representations, as most machine learning algorithms require numerical input data. We will explore various encoding techniques, such as one-hot encoding, label encoding, and target encoding, to transform categorical variables into a format that can be readily utilized by our models.\n\nBy appropriately encoding categorical variables, we ensure that valuable information captured by these features is effectively incorporated into our predictive models, ultimately enhancing their performance and predictive accuracy.\ng","metadata":{}},{"cell_type":"code","source":"df6 = df5.copy() \ndf_test6 = df_test5.copy()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.885139Z","iopub.execute_input":"2024-04-09T15:14:38.885796Z","iopub.status.idle":"2024-04-09T15:14:38.892534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df5_b = pd.get_dummies(df5_b) # Base DataFrame After Encoding, Before Scaling","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.89362Z","iopub.execute_input":"2024-04-09T15:14:38.893911Z","iopub.status.idle":"2024-04-09T15:14:38.938234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf6 = pd.get_dummies(df6) # Final Model DataFrame Sequence CheckPoint After Encoding, Before Scaling","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.939473Z","iopub.execute_input":"2024-04-09T15:14:38.939875Z","iopub.status.idle":"2024-04-09T15:14:38.981031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test6 = pd.get_dummies(df_test6) # Test DataFrame Sequence CheckPoint After Encoding, Before Scaling","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:38.982272Z","iopub.execute_input":"2024-04-09T15:14:38.983755Z","iopub.status.idle":"2024-04-09T15:14:39.024726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df6.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.025972Z","iopub.execute_input":"2024-04-09T15:14:39.027027Z","iopub.status.idle":"2024-04-09T15:14:39.035925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df5_b.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.037294Z","iopub.execute_input":"2024-04-09T15:14:39.037859Z","iopub.status.idle":"2024-04-09T15:14:39.04388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test6.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.04512Z","iopub.execute_input":"2024-04-09T15:14:39.046013Z","iopub.status.idle":"2024-04-09T15:14:39.055646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We will drop 18 columns to increase consistency in our prediction","metadata":{}},{"cell_type":"code","source":"# Get the columns that are present in df6 but not in df_test6\nmissing_columns_df_test6 = df6.columns.difference(df_test6.columns)\n\n# Get the columns that are present in df_test6 but not in df6\nmissing_columns_df6 = df_test6.columns.difference(df6.columns)\n\n# Concatenate the two sets of columns\ndifferent_columns = missing_columns_df_test6.union(missing_columns_df6)\n\n# Drop the columns that are different between df6, df_test6 and df5_b\ndf6 = df6.drop(columns=different_columns, errors='ignore')\ndf_test6 = df_test6.drop(columns=different_columns, errors='ignore')\ndf5_b = df5_b.drop(columns=different_columns, errors='ignore')\n\n\n# Check again for any remaining differences\ndifferent_columns_after_drop = df6.columns.difference(df_test6.columns).union(df_test6.columns.difference(df6.columns))\n\nprint(\"Columns that are different between df6 and df_test6 after dropping:\")\nprint(different_columns_after_drop)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.056777Z","iopub.execute_input":"2024-04-09T15:14:39.057055Z","iopub.status.idle":"2024-04-09T15:14:39.070536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df6.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.071733Z","iopub.execute_input":"2024-04-09T15:14:39.07201Z","iopub.status.idle":"2024-04-09T15:14:39.078384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df5_b.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.079625Z","iopub.execute_input":"2024-04-09T15:14:39.080544Z","iopub.status.idle":"2024-04-09T15:14:39.088706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test6.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.089857Z","iopub.execute_input":"2024-04-09T15:14:39.090544Z","iopub.status.idle":"2024-04-09T15:14:39.099805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DataFrames are all fixed to the same number of columns.","metadata":{}},{"cell_type":"markdown","source":"# Scaling","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(df6) # Scale df6\n\ndf6 = pd.DataFrame(scaler.transform(df6), index=df6.index, columns = df6.columns) # We can scale this way since this is a competition data set. Production data sets requires a different approach.","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.101214Z","iopub.execute_input":"2024-04-09T15:14:39.101705Z","iopub.status.idle":"2024-04-09T15:14:39.137069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(df_test6) # Scale df_test6\n\ndf_test6 = pd.DataFrame(scaler.transform(df_test6), index=df_test6.index, columns = df_test6.columns) # We can scale this way since this is a competition data set. Production data sets requires a different approach.","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.138467Z","iopub.execute_input":"2024-04-09T15:14:39.139094Z","iopub.status.idle":"2024-04-09T15:14:39.172031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(df5_b) # Scale df5_b\n\ndf5_b = pd.DataFrame(scaler.transform(df5_b), index=df5_b.index, columns = df5_b.columns) # We can scale this way since this is a competition data set. Production data sets requires a different approach.","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.173409Z","iopub.execute_input":"2024-04-09T15:14:39.17404Z","iopub.status.idle":"2024-04-09T15:14:39.205211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target Transform","metadata":{}},{"cell_type":"markdown","source":"### Check and Optimize Skeweness for Target","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\n\nplt.subplot(1, 2, 1)\nsns.distplot(target, kde=True, fit=scipy.stats.norm) # Chart with target\nplt.title(\"Without Log Transform\")\n\nplt.subplot(1, 2, 2)\nsns.distplot(np.log(target), kde=True, fit=scipy.stats.norm) # Chart with np.log(target)\nplt.xlabel(\"Log SalePrice\")\nplt.title(\"With Log Transform\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.206617Z","iopub.execute_input":"2024-04-09T15:14:39.207232Z","iopub.status.idle":"2024-04-09T15:14:39.975329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transform target to logarithmic scale for improved model accuracy","metadata":{}},{"cell_type":"code","source":"log_target = np.log(target)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.978301Z","iopub.execute_input":"2024-04-09T15:14:39.978818Z","iopub.status.idle":"2024-04-09T15:14:39.983983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Selection","metadata":{}},{"cell_type":"code","source":"df7 = df6.copy()\ndf_test7 = df_test6.copy()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.985315Z","iopub.execute_input":"2024-04-09T15:14:39.98568Z","iopub.status.idle":"2024-04-09T15:14:39.995605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# _base = setup(data=pd.concat([df5_b, target], axis = 1), target='SalePrice') # Setup Base Model for Compare_Models()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:39.996876Z","iopub.execute_input":"2024-04-09T15:14:39.99733Z","iopub.status.idle":"2024-04-09T15:14:40.003539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare_models() # Base Model Results - Original Scale","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:40.004884Z","iopub.execute_input":"2024-04-09T15:14:40.005441Z","iopub.status.idle":"2024-04-09T15:14:40.013349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Base Model Comparison Results\n\n| Model   | Algorithm                        | MAE     | MSE            | RMSE     | R2      | RMSLE   | MAPE    | TT (Sec) |\n|---------|----------------------------------|---------|----------------|----------|---------|---------|---------|----------|\n| rf      | Random Forest Regressor          | 17465.71| 883535086.6411 | 28643.82 | 0.8588  | 0.1423  | 0.1018  | 3.0860   |\n| en      | Elastic Net                      | 17442.32| 969921307.9781 | 30608.55 | 0.8442  | 0.1471  | 0.0990  | 0.1710   |\n| et      | Extra Trees Regressor            | 18084.11| 1016922336.7136| 30753.60 | 0.8381  | 0.1451  | 0.1037  | 3.1400   |\n| llar    | Lasso Least Angle Regression     | 17330.21| 1068095990.7947| 31516.54 | 0.8256  | 0.1694  | 0.0990  | 0.1820   |\n| br      | Bayesian Ridge                   | 18109.04| 1078690939.5830| 32087.31 | 0.8234  | 0.1806  | 0.1055  | 0.2920   |\n| omp     | Orthogonal Matching Pursuit      | 18848.16| 1114194713.1411| 32375.81 | 0.8177  | 0.1878  | 0.1114  | 0.1370   |\n| ada     | AdaBoost Regressor               | 23403.54| 1223525884.9374| 34212.08 | 0.8047  | 0.1921  | 0.1512  | 0.5380   |\n| huber   | Huber Regressor                  | 18654.49| 1255935152.4894| 34199.43 | 0.7915  | 0.2120  | 0.1117  | 0.5150   |\n| par     | Passive Aggressive Regressor     | 20487.04| 1590317308.9775| 38851.88 | 0.7479  | 0.2669  | 0.1252  | 1.0510   |\n| ridge   | Ridge Regression                 | 20367.30| 1577426230.5234| 38186.03 | 0.7375  | 0.2450  | 0.1218  | 0.1660   |\n| knn     | K Neighbors Regressor           | 25504.64| 1838030835.5396| 42283.13 | 0.7174  | 0.1947  | 0.1404  | 0.0790   |\n| dt      | Decision Tree Regressor         | 27715.41| 2081641669.8858| 44186.93 | 0.6720  | 0.2115  | 0.1545  | 0.1240   |\n| lasso   | Lasso Regression                | 21641.85| 3426809426.8291| 48484.40 | 0.4377  |         |         |          |\n\n### Summary and Analysis\n\nBaseline model comparisons show the performance of different regression algorithms. Among these models, Random Forest Regressor (rf) stands out with the lowest Mean Absolute Error (MAE), Mean Square Error (MSE) and Root Mean Square Error (RMSE), meaning it makes better predictions and fewer errors than others. It also provides a high R-squared (R2) value, indicating a good fit to the data. With these features, the Random Forest Regressor is a strong candidate as the final model. However, additional factors such as the computational efficiency and interpretability of the model need to be evaluated before making a definitive decision.\n","metadata":{}},{"cell_type":"code","source":"# _ = setup(data=pd.concat([df7, log_target], axis = 1), target='SalePrice') # Setup Final Model for Compare_Models()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-09T15:14:40.014793Z","iopub.execute_input":"2024-04-09T15:14:40.015146Z","iopub.status.idle":"2024-04-09T15:14:40.022792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare_models() # Final Model Logarithmic Scale Results","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:40.024296Z","iopub.execute_input":"2024-04-09T15:14:40.024638Z","iopub.status.idle":"2024-04-09T15:14:40.031592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final Model Comparison Results\n\n| Model   | Algorithm                          | MAE   | MSE   | RMSE  | R2    | RMSLE | MAPE  | TT (Sec) |\n|---------|------------------------------------|-------|-------|-------|-------|-------|-------|----------|\n| catboost| CatBoost Regressor                 | 0.0784| 0.0143| 0.1175| 0.9092| 0.0091| 0.0066| 4.1710   |\n| gbr     | Gradient Boosting Regressor        | 0.0881| 0.0173| 0.1295| 0.8907| 0.0101| 0.0074| 1.0190   |\n| lightgbm| Light Gradient Boosting Machine    | 0.0918| 0.0179| 0.1323| 0.8865| 0.0103| 0.0077| 0.3560   |\n| br      | Bayesian Ridge                     | 0.0885| 0.0186| 0.1343| 0.8807| 0.0104| 0.0074| 0.2020   |\n| rf      | Random Forest Regressor            | 0.0962| 0.0195| 0.1379| 0.8767| 0.0107| 0.0081| 3.0140   |\n| et      | Extra Trees Regressor              | 0.0965| 0.0197| 0.1391| 0.8755| 0.0108| 0.0081| 3.1460   |\n| xgboost | Extreme Gradient Boosting          | 0.0983| 0.0208| 0.1424| 0.8685| 0.0111| 0.0082| 0.8880   |\n| omp     | Orthogonal Matching Pursuit        | 0.0954| 0.0223| 0.1472| 0.8575| 0.0114| 0.0080| 0.1310   |\n| ridge   | Ridge Regression                   | 0.0947| 0.0233| 0.1503| 0.8500| 0.0118| 0.0079| 0.1520   |\n| ada     | AdaBoost Regressor                 | 0.1274| 0.0292| 0.1698| 0.8154| 0.0132| 0.0106| 0.5760   |\n| knn     | K Neighbors Regressor              | 0.1400| 0.0389| 0.1966| 0.7519| 0.0152| 0.0117| 0.0840   |\n| dt      | Decision Tree Regressor            | 0.1472| 0.0441| 0.2077| 0.7192| 0.0162| 0.0123| 0.1200   |\n| lasso   | Lasso Regression                   | 0.3080| 0.1582| 0.3973| -0.0101| 0.0305| 0.0256| 0.1340   |\n\n### Summary and Analysis\n\nBased on the model comparison results, several algorithms have performed well in terms of metrics such as MAE, MSE, RMSE, R2, RMSLE, and MAPE. The CatBoost Regressor stands out as it has the lowest MAE, MSE, RMSE, and RMSLE values, along with the highest R2 value among all models, indicating better predictive performance. Additionally, the Bayesian Ridge model also shows promising results with relatively low error metrics and a high R2 value. However, it's important to consider other factors such as computational efficiency (TT - Time Taken) and ease of implementation. In this context, the Bayesian Ridge model might be preferred due to its good balance between performance and computational efficiency.\n","metadata":{}},{"cell_type":"code","source":"final = BayesianRidge(verbose=0)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:40.032709Z","iopub.execute_input":"2024-04-09T15:14:40.032958Z","iopub.status.idle":"2024-04-09T15:14:40.041253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final.fit(df7, log_target)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:40.04235Z","iopub.execute_input":"2024-04-09T15:14:40.042873Z","iopub.status.idle":"2024-04-09T15:14:40.250031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# K-Fold Cross Validation \nIn this section, we perform 10-split k-fold cross-validation with the final model, possibly Bayesian Ridge, on the preprocessed dataset (df7) with logarithmically transformed target variable.\n\nFirst, we standardize feature names in df7 by using underscores instead of spaces, thus ensuring consistency across the dataset.\n\nCross-validation results focusing on negative mean squared error (neg_mean_squared_error) are stored in the 'results' variable. Using 10-bin KFold, we evaluate the generalization performance of the model by repeatedly splitting it into training and validation sets. Lower negative mean square error values indicate the superior predictive ability of the model and provide insight into the stability of the model and its generalization capacity to new data.\n\nThis method provides valuable information about reliability and generalization ability by comprehensively evaluating the model's performance on different data partitions.\n","metadata":{}},{"cell_type":"markdown","source":"### Final Model with Cross Validation","metadata":{}},{"cell_type":"code","source":"cross_val_score(final, df7, log_target, scoring='neg_mean_squared_error')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:40.252449Z","iopub.execute_input":"2024-04-09T15:14:40.254074Z","iopub.status.idle":"2024-04-09T15:14:40.865045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace whitespace in feature names with underscores\ndf7.columns = df7.columns.str.replace(' ', '_')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:40.868995Z","iopub.execute_input":"2024-04-09T15:14:40.871616Z","iopub.status.idle":"2024-04-09T15:14:40.879645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_val_score(final, df7, log_target, scoring='neg_mean_squared_error')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:40.882099Z","iopub.execute_input":"2024-04-09T15:14:40.883666Z","iopub.status.idle":"2024-04-09T15:14:41.585176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KFold For Final Model","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=10)\n\nresults = cross_val_score(final, df7, log_target, scoring='neg_mean_squared_error', cv=kf)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:41.587472Z","iopub.execute_input":"2024-04-09T15:14:41.589009Z","iopub.status.idle":"2024-04-09T15:14:43.138081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:43.143007Z","iopub.execute_input":"2024-04-09T15:14:43.146468Z","iopub.status.idle":"2024-04-09T15:14:43.160378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction Part\n\nIn this last part, we prepare the model for shipping by making predictions on the test data set (df_test7). First, we extract the 'Id' column from the test dataset and store it in the 'test_ids' variable.\n\nThen, we make predictions using the final model on the processed test dataset and transform the predicted SalePrice values to the original scale by applying the exponential function (np.exp).\n\nPredictions are concatenated into a DataFrame named 'post' using the 'pd.concat' function with the corresponding 'Id' values. This DataFrame provides the submission format required for the contest.\n\nFinally, this DataFrame is saved into a CSV file named 'submission.csv' with the 'to_csv' function. This file is a contest reference file containing estimated SalePrice values for the test dataset, with no index and header.\n","metadata":{}},{"cell_type":"code","source":"test_ids = df_test['Id']\ntest_ids","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-09T15:14:43.187357Z","iopub.execute_input":"2024-04-09T15:14:43.191394Z","iopub.status.idle":"2024-04-09T15:14:43.209184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict test","metadata":{}},{"cell_type":"code","source":"predictions = np.exp(final.predict(df_test7))\n\nsubmission = pd.concat([test_ids, pd.Series(predictions, name='SalePrice')], axis=1)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:43.212831Z","iopub.execute_input":"2024-04-09T15:14:43.213966Z","iopub.status.idle":"2024-04-09T15:14:43.248978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('./submission.csv', index = False, header = True)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:14:43.25287Z","iopub.execute_input":"2024-04-09T15:14:43.255487Z","iopub.status.idle":"2024-04-09T15:14:43.272271Z"},"trusted":true},"execution_count":null,"outputs":[]}]}